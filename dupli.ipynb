{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d08025",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4aa3db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_description=docx2txt.process(\"jobdescription.docx\")\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38410e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03851a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbea7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfminer.six\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3405da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as ps\n",
    "import os\n",
    "import docx2txt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import docx2txt\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees={'bachelor of technology','b.tech','b tech','btech','engineering'\n",
    "         'bachelor of arts','b.a','b a','master'\n",
    "         'bachelor of science','bsc',\n",
    "         'b e','b.e.', 'b.e', 'bs', 'b.s',\n",
    "         'bachelor of Commerce'\n",
    "         'm e', 'm.e', 'm.e.', 'm.s', 'm.s', \n",
    "         'm.tech', 'mtech'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfminer.six\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97afd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import utils\n",
    "from datetime import date\n",
    "j=1\n",
    "directory=r'C:\\Users\\91994\\fres'\n",
    "for file in os.listdir(directory):\n",
    "    data=ResumeParser(file).get_extracted_data()\n",
    "    resume=docx2txt.process(file)\n",
    "    text=[resume,job_description]\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cv=CountVectorizer()\n",
    "    count_matrix=cv.fit_transform(text)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    matchpercentage=cosine_similarity(count_matrix)[0][1]*100\n",
    "    matchpercentage=round(matchpercentage,2)\n",
    "    if(matchpercentage>50):\n",
    "        data=ResumeParser(file).get_extracted_data()\n",
    "        list=data[\"name\"]\n",
    "        list1=data[\"email\"]\n",
    "        list2=data[\"mobile_number\"]\n",
    "        list3=data[\"skills\"]\n",
    "        list4=data[\"degree\"]\n",
    "        list6=data[\"experience\"]\n",
    "        list5=data[\"total_experience\"]\n",
    "        def get_degree(input_text):\n",
    "\n",
    "            word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "            filtered_tokens = [w for w in word_tokens]\n",
    "\n",
    "            found_degree = set()\n",
    "            for token in filtered_tokens:\n",
    "                if token.lower() in degrees:\n",
    "                    found_degree.add(token)\n",
    "\n",
    "            bigrams_trigrams = map(' '.join, nltk.everygrams(filtered_tokens, 2, 3))\n",
    "            for ngram in bigrams_trigrams:\n",
    "                if ngram.lower() in degrees:\n",
    "                    found_degree.add(ngram)\n",
    "            return found_degree\n",
    "        text=ResumeParser(file).get_extracted_data()\n",
    "        resume=docx2txt.process(file)\n",
    "        list7=get_degree(resume)\n",
    "        def get_experience(resume_text):\n",
    "\n",
    "            text = resume_text.lower()\n",
    "\n",
    "            def monthnum(text):\n",
    "                month_dict = {'jan': '01','feb': '02','mar': '03','apr': '04','may': '05','jun': '06',\n",
    "                    'jul': '07','aug': '08','sep': '09','oct': '10','nov': '11','dec': '12',\n",
    "                    'january':'01','february': '02','march': '03','april': '04','may': '05','june': '06',\n",
    "                    'july': '07','august': '08','september': '09','october': '10','november': '11','december': '12'}\n",
    "  \n",
    "                return ' '.join( map(lambda x:month_dict[x] if x in month_dict.keys() else x,re.split('[^a-zA-Z0-9]',text)) )\n",
    "\n",
    "            text = monthnum(text)\n",
    "    \n",
    "\n",
    "            op_lis = []\n",
    "\n",
    "            stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "            word_tokens = nltk.tokenize.word_tokenize(text)\n",
    "            filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "            ps = PorterStemmer()\n",
    "            kwy_words = ['experience','workhistory','exposure','experience']\n",
    "    \n",
    "            indx = 0\n",
    "            for i in range(len(filtered_tokens)):\n",
    "                if ps.stem(filtered_tokens[i]) == 'experi':\n",
    "                    indx = i\n",
    "                    break\n",
    "                elif ps.stem(filtered_tokens[i]) == 'histori' or ps.stem(filtered_tokens[i]) == 'exposur':\n",
    "                    if ps.stem(filtered_tokens[i-1]) == 'work' or ps.stem(filtered_tokens[i-1]) == 'industri':\n",
    "                        index = i\n",
    "                        break\n",
    "\n",
    "            filtered_tokens = filtered_tokens[i:]\n",
    "   \n",
    "    \n",
    "    \n",
    "            ngrams = map(' '.join, nltk.everygrams(filtered_tokens, 2, 4))\n",
    "            text = '-'.join(ngrams)\n",
    "            my_REG = re.compile(r'[0-9]{2}[ ]{0,}20[0-9]{2}[ ]{0,}[0-9]{2}[ ]{0,}20[0-9]{2}')\n",
    "\n",
    "            month_year = re.findall(my_REG,text)\n",
    "\n",
    "            month_year =  set(map(lambda x:x.strip(),month_year))\n",
    "\n",
    "            op_lis.extend(set(month_year))\n",
    "\n",
    "\n",
    "\n",
    "            if len(month_year) == 0:\n",
    "\n",
    "                y_REG = re.compile(r'20[0-9]{2}[ ]{0,}20[0-9]{2}')\n",
    "\n",
    "                year = re.findall(y_REG,text)\n",
    "\n",
    "                year = set(map(lambda x:x.strip(),year))\n",
    "\n",
    "                op_lis.extend(set(year))\n",
    "\n",
    "\n",
    "\n",
    "            my_REG = re.compile(r'[0-9]{2}[ ]{0,}20[0-9]{2}[ ]{0,}now|[0-9]{2}[ ]{0,}20[0-9]{2}[ ]{0,}current|[0-9]{2}[ ]{0,}20[0-9]{2}[ ]{0,}present')\n",
    "\n",
    "            month_year = re.findall(my_REG,text)\n",
    "\n",
    "            month_year = set(map(lambda x:x.strip(),month_year))\n",
    "\n",
    "            op_lis.extend(set(month_year))\n",
    "\n",
    "    \n",
    "\n",
    "            if len(month_year) == 0:\n",
    "\n",
    "                y_REG = re.compile(r'20[0-9]{2}[ ]{0,}now|20[0-9]{2}[ ]{0,}current|20[0-9]{2}[ ]{0,}present')\n",
    "\n",
    "                year = re.findall(y_REG,text)\n",
    "\n",
    "                year = set(map(lambda x:x.strip(),year))\n",
    "\n",
    "                op_lis.extend(set(year))\n",
    "            return op_lis\n",
    "        text=ResumeParser(file).get_extracted_data()\n",
    "        resume=docx2txt.process(file)\n",
    "        list8=get_experience(resume)\n",
    "        def diff(date1, date2):\n",
    "\n",
    "            return (date2-date1).days\n",
    "\n",
    "\n",
    "\n",
    "        op = []\n",
    "\n",
    "        for i in list8:\n",
    "\n",
    "            if len(i) == 15:\n",
    "\n",
    "                if i[-7:] == 'present':\n",
    "  \n",
    "                    prev_date = date(int(i[3:7]),int(i[:2]),1)\n",
    "\n",
    "                    curr_date = date.today()\n",
    "\n",
    "                    op.append(diff(prev_date,curr_date))\n",
    "\n",
    "            else:\n",
    "\n",
    "                prev_date = date(int(i[3:7]),int(i[:2]),1)\n",
    "\n",
    "                curr_date = date(int(i[-4:]),int(i[-7:-5]),1)\n",
    "\n",
    "                op.append(diff(prev_date,curr_date))\n",
    "\n",
    "        list9=sum(op)/365\n",
    "        list10=int(list9)\n",
    "        import pandas as pd\n",
    "        dict={'name':list,'email':list1,'phone':list2,'skills':[list3],'degree':list7,'experience':list10}\n",
    "        df=pd.DataFrame([dict],index=[j])\n",
    "      \n",
    "        if j>1:\n",
    "            df = pd.concat([df1, df]).reset_index(drop=True)\n",
    "        df1=df\n",
    "        j=j+1\n",
    "df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ca4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbf985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df.to_excel('C:\\\\Users\\\\91994\\\\OneDrive\\\\Desktop\\\\mounika\\\\dfres1.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
